# Mushroom Classification

![](/presentation_images/mushrooms.png?raw=true "Example Images")
## Introduction


As any wild mushroom forager knows, mushroom identification can often be difficult and time consuming. Currently, several apps exist to address this problem: most of them allow a user to upload a picture and then return a prediction of the mushroom's variety or edibility. While many are fairly accurate and can be useful tools, the flaw that these apps all share is their reliance on the internet.

Most applications for image or speech recognition run on neural networks. The average neural network uses more computing power than a smartphone is capable of, so these applications relay the data (e.g., the image, your voice) via internet to external servers for processing. Since most mushroom foraging takes place in the woods, where cell service is inconsistent at best, current mushroom identification apps do not meet the needs of foragers who wish to identify their finds on the go.

My objective was to create a mushroom image classifier light-weight enough to be operated on a smartphone, without the need for external servers. 

## Data

The dataset for this project consisted of labeled mushroom images I scraped from four mycology websites and Google image searches. Before cleaning there were over 3,400 edible mushroom images and about 2,500 poisonous images. For more details on data collection, see the **scrapers** folder 

To ensure my model was trained on useful data, I removed any image that was too small, had poor resolution or had a main forus that was not a mushroom. After cleaning there were about 3,100 edible images and 2,250 poisonous images. Below are some examples of images that were removed from the dataset.

![](/presentation_images/removed.png?raw=true)

## Methodology
Since deep learning models perform best when they have a significant amount of data to learn from, and my dataset was fairly small, the first step of this project was to determine a strategy for optimizing my data. The first technique I used was data augmentation - slightly altering each training image in order to generate additional training images. The example below shows images that were generated by applying data augmentation to the same image multiple times.

![](/presentation_images/augmentation.png?raw=true)

The second strategy I used was transfer learning. This technique essentially uses a pre-trained model as the starting point for a new model, allowing the new model to use information that the pre-trained model learned. 

Google’s MobileNetV2 was the selection I made for the pre-trained base of my model. The reason I made this choice is that MobileNetV2 was created specifically for use on smartphones, so it is a good fit for my end goal of a product that can operate without external servers.

I tried three different approaches to model design:

* Model A - Data augmentation but no transfer learning. I wanted to see how well a model trained from scratch would perform to use a comparison with later models. 

* Model B - Data augmentation and transfer learning. In this model I froze the entire convolutional base of the MobileNetV2 model, meaning that none of the pre-trained model’s weights were updated during training. 

* Model C - Data augmentation and transfer learning. This model froze some of the base model’s layers and trained about 30% of them. This approach is known as ‘fine tuning’ and can usually be expected to provide the best results. 

## Results

As anticipated, Model C outperformed the other two models. Model A reached 70% accuracy on the validation data. Model B’s validation accuracy was also 70%, but it showed slightly less overfitting than Model A. Model C’s validation accuracy was 75%, and reached 81% on the test data. Below are some predictions Model C made on the previously unseen test data:

![](/presentation_images/predictions.png?raw=true)
